{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of OCR Envision",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/govoyager/ENVISION_PROJECT/blob/main/Copy_of_OCR_Envision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrwEV78xuWul"
      },
      "source": [
        "# Optical Character Recognition using Convolutional Neural Networks\n",
        "\n",
        "This notebook contains the code used to load character datasets from the internet, preprocess the data and train a CNN model to predict the character labels. We also take a real world image of handwritten characters to observe how our model fits to real world data. The full project report can be found [here.](https://ieee.nitk.ac.in/virtual-expo)\n",
        "\n",
        "## References\n",
        "- [A-Z Handwritten Alphabets in .csv format | Kaggle](https://www.kaggle.com/sachinpatel21/az-handwritten-alphabets-in-csv-format)\n",
        "- [MNIST digits classification dataset | Keras](https://keras.io/api/datasets/mnist/)\n",
        "- [Keras Layers API](https://keras.io/api/layers/#layers-api-overview)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJCD_nvoIer1"
      },
      "source": [
        "# Importing the MNIST Dataset\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Importing data manipulation and visualisation libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS7bzPeX28oH"
      },
      "source": [
        "# 1. Loading the datasets\n",
        "\n",
        "The MNIST Dataset is simply loaded in using the `load_data` function provided by the Keras dataset.\n",
        "\n",
        "We use the [Kaggle API](https://github.com/Kaggle/kaggle-api) to download the A-Z Handwritten Alphabets dataset into the Colab storage. In order to use the Kaggle API, an API Key needs to be generated and uploaded to the Colab environment. To create your own API Key, follow the steps mentioned [here.](https://github.com/Kaggle/kaggle-api#api-credentials)\n",
        "\n",
        "The dataset is downloaded and then loaded into a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9oVo75wLPPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30cf60f9-8c2b-4e64-f784-573766a66be1"
      },
      "source": [
        "# Load the MNIST Number dataset\n",
        "(train_X_mn, train_y_mn), (test_X_mn, test_y_mn) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "v4yBiznqNqhJ",
        "outputId": "8e5767a4-cdae-4c98-846d-c4cc0fad3cd9"
      },
      "source": [
        "# Displaying random images from the training dataset\n",
        "mn_idx = np.random.randint(0, train_X_mn.shape[0])\n",
        "plt.imshow(train_X_mn[mn_idx])\n",
        "plt.title(train_y_mn[mn_idx])\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "oO8Nr0H_LK5w",
        "outputId": "7fe892d9-169c-489f-b024-0a78446e05be"
      },
      "source": [
        "# Kaggle API Key import\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c1be153b-3615-4e7b-be02-909fdbb28b3c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c1be153b-3615-4e7b-be02-909fdbb28b3c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzUXGzAOJynh"
      },
      "source": [
        "# Load the dataset from Kaggle\n",
        "!kaggle datasets download \"sachinpatel21/az-handwritten-alphabets-in-csv-format\"\n",
        "!unzip az-handwritten-alphabets-in-csv-format.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGIVLyQQO38B"
      },
      "source": [
        "# Load the A-Z Handwritten dataset into a Pandas DataFrame\n",
        "# This takes time (and RAM usage) because the dataset is 666.53MB in size\n",
        "_X_az = pd.read_csv('/content/A_Z Handwritten Data.csv')\n",
        "_X_az.rename(columns={'0':'label'}, inplace=True)\n",
        "\n",
        "# Split the dataset into the target variable and training data\n",
        "y_az = _X_az.label.to_numpy()\n",
        "X_az = _X_az.drop('label', axis=1).to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WFGnkRxXq06"
      },
      "source": [
        "# Displaying random images from the A-Z Handwritten dataset\n",
        "az_idx = np.random.randint(0, X_az.shape[0])\n",
        "plt.imshow(X_az[az_idx].reshape((28,28)))\n",
        "plt.title(chr(y_az[az_idx] + 65))\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0aToMu6o36"
      },
      "source": [
        "# 2. Data Preprocessing\n",
        "\n",
        "We first split the A-Z Handwritten Alphabets Dataset into train and test sets. The MNIST Digits Dataset is already split into train and test sets.\n",
        "\n",
        "We then scale our data using the Standard Scaler provided by scikit-learn. In order to scale the data, we first reshape each 28 x 28 image in the MNIST Digits Dataset into a long 784 dimensional vector.\n",
        "\n",
        "Since we will be compiling the datasets together, we increment the A-Z Handwritten Alphabets Dataset labels by 10. Therefore, labels 0-9 represent the digits 0-9, and labels 10-35 represent the characters A-Z.\n",
        "\n",
        "We also apply one-hot encoding on the target variable in order to align the target variable with the output of the CNN model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbo2n5L0PX0C"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and testing data\n",
        "train_X_az, test_X_az, train_y_az, test_y_az = train_test_split(X_az, y_az, test_size = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQeN-ut1-a95"
      },
      "source": [
        "# Adding 10 to every character label\n",
        "train_y_az += 10\n",
        "test_y_az += 10\n",
        "\n",
        "# Reshaping the MNIST Digits Dataset to a set of 784 dimensional vectors \n",
        "train_X_mn = train_X_mn.reshape((train_X_mn.shape[0], 784))\n",
        "test_X_mn = test_X_mn.reshape((test_X_mn.shape[0], 784))\n",
        "\n",
        "# Compiling the MNIST Dataset and the A-Z Handwritten Dataset\n",
        "train_X = np.vstack((train_X_mn, train_X_az))\n",
        "train_y = np.hstack((train_y_mn, train_y_az))\n",
        "\n",
        "test_X = np.vstack((test_X_mn, test_X_az))\n",
        "test_y = np.hstack((test_y_mn, test_y_az))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB1TYU1VRFfW"
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "# Converting the outputs to a one hot encoding (which will be used to predict the output for the CNN)\n",
        "train_y_ohc = to_categorical(train_y, num_classes=36, dtype=int)\n",
        "test_y_ohc = to_categorical(test_y, num_classes=36, dtype=int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQjrVnUEB5SF"
      },
      "source": [
        "### **Note**\n",
        "\n",
        "At this point, with all the different variables we have created, the RAM usage starts nearing ~10GB. Further computation cannot be done using the free tier of Colab, because the RAM usage will exceed 12.69GB (total RAM allocated for the free tier). Thus, we save the existing dataset into Google Drive and restart the runtime. This refreshes the RAM usage so that we can execute the remaining cells in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-CAzATlBr-I"
      },
      "source": [
        "# Save the dataset (including train and test) into a numpy archive (.npz)\n",
        "np.savez(\"dataset_unscaled.npz\", train_X=train_X, train_y=train_y_ohc, test_X=test_X, test_y=test_y_ohc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAISZIgTC5eW"
      },
      "source": [
        "# Kill the current Python runtime process, which will cause the Colab runtime to restart\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm3dXO_5DNlw"
      },
      "source": [
        "# Import the necessary libraries after restarting the runtime\n",
        "# If the runtime is not being restarted, this cell can be skipped\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLtOWsC4DjnA"
      },
      "source": [
        "# Reload the data from the saved numpy archive\n",
        "dataset = np.load(\"dataset_unscaled.npz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztlUH-EWmqro"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scaling the data\n",
        "scaler = StandardScaler()\n",
        "train_X_sc = scaler.fit_transform(dataset[\"train_X\"])\n",
        "test_X_sc = scaler.transform(dataset[\"test_X\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWTDIJVBEJAM"
      },
      "source": [
        "# Reshape the data into an array of (28,28,1) images\n",
        "train_X = train_X_sc.reshape((train_X_sc.shape[0], 28, 28, 1))\n",
        "test_X = test_X_sc.reshape((test_X_sc.shape[0], 28, 28, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uZmtlFZEi_G"
      },
      "source": [
        "# 3. Creating and Training the CNN Model\n",
        "\n",
        "We create a simple CNN model using layers provided by Keras. We then train this model on the scaled data using the Adam optimizer.\n",
        "\n",
        "The training and validation losses and accuracies are visualized using line graphs provided by matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQPjXCUxAZO6"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Defining the model architecture\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', input_shape=(28,28,1)))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=2))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=2))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(192, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(36, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSabJ95yJRfN"
      },
      "source": [
        "# Fitting the model to the training data and validating using testing data\n",
        "history = model.fit(train_X, dataset['train_y'], epochs=8, batch_size=128, verbose=1, validation_data=(test_X, dataset['test_y']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoL603zmK45E"
      },
      "source": [
        "# Print out the loss and accuracy scores\n",
        "scores = model.evaluate(test_X, dataset['test_y'], verbose=0)\n",
        "print('Validation Loss : {:.4f}'.format(scores[0]))\n",
        "print('Validation Accuracy: {:.4f}'.format(scores[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg5Ls9zpUjmR"
      },
      "source": [
        "# Create 2 subplots to plot the loss and accuracy curves\n",
        "plt.figure()\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(19,7))\n",
        "\n",
        "# Plot Loss function values over epochs for training and validation sets\n",
        "ax1.plot(history.history['loss'])\n",
        "ax1.plot(history.history['val_loss'])\n",
        "ax1.legend(['Training Loss','Validation Loss'])\n",
        "ax1.set_title('Loss')\n",
        "ax1.set_xlabel('epochs')\n",
        "\n",
        "# Plot accuracy over epochs for training and validation sets\n",
        "ax2.plot(history.history['accuracy'])\n",
        "ax2.plot(history.history['val_accuracy'])\n",
        "ax2.legend(['Training Accuracy','Validation Accuracy'])\n",
        "ax2.set_title('Accuracy')\n",
        "ax2.set_xlabel('epochs')\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FPXah5pI5C-"
      },
      "source": [
        "# 4. Predicting Characters on Real World Data\n",
        "\n",
        "We now use image processing to extract the characters from an image of handwritten alphabets. We make use of image processing libraries like OpenCV to find the contours and reshape the character images to the exact dimensions required.\n",
        "\n",
        "Once we extract the characters, we pass them to the model for it to predict the character label. We finally write the results back on to the image as our final result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VIe8awYwMY5"
      },
      "source": [
        "import cv2\n",
        "import imutils\n",
        "from imutils.contours import sort_contours\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load in our image\n",
        "img = cv2.imread(\"abc.jpeg\")\n",
        "img = cv2.copyMakeBorder(img, top = 40, bottom=0, left=0, right = 40, borderType = cv2.BORDER_CONSTANT, value=(255,255,255))\n",
        "\n",
        "# Convert to B/W (single channel)\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply blur to reduce noise and smooth out the edges\n",
        "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "# Apply Canny edge detection to find all the edges of the image\n",
        "edged = cv2.Canny(blurred, 30, 150)\n",
        "\n",
        "# Find all the contours, retrieve only external contours and store only corner points\n",
        "cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Grab all contours \n",
        "cnts = imutils.grab_contours(cnts)\n",
        "# cnts = sort_contours(cnts, method=\"left-to-right\")[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-8_Fmq-e_X_"
      },
      "source": [
        "cv2_imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb9gMStGaPqW"
      },
      "source": [
        "cv2_imshow(edged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO7-UfueeDMV"
      },
      "source": [
        "chars = []\n",
        "for c in cnts:\n",
        "    # Find the bounding rectangle of the contour\n",
        "    (x ,y, w, h) = cv2.boundingRect(c)\n",
        "\n",
        "    # Eliminate misinterpreted contours\n",
        "    if (w >= 5 and w <= 150) and (h >= 15 and h <= 120):\n",
        "        # Extract the Region of Interest\n",
        "        roi = gray[y:y + h, x:x + w]\n",
        "\n",
        "        # Since the original images have white background, invert the colors using inverted threshold\n",
        "        thresh = cv2.threshold(roi, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
        "        (tH, tW) = thresh.shape\n",
        "\n",
        "        # Resize image wrt one dimension\n",
        "        if tW > tH:\n",
        "            thresh = imutils.resize(thresh, width=28)\n",
        "        else:\n",
        "            thresh = imutils.resize(thresh, height=28)\n",
        "\n",
        "        # Fetch new shape\n",
        "        (tH, tW) = thresh.shape\n",
        "\n",
        "        # Find the total space needed to fill as border\n",
        "        dX = int(max(0, 28 - tW) / 2.0)\n",
        "        dY = int(max(0, 28 - tH) / 2.0)\n",
        "\n",
        "        # Add a border if the image is smaller than necessary, using a constant black border\n",
        "        padded = cv2.copyMakeBorder(thresh, top=dY, bottom=dY, left=dX, right=dX, borderType=cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
        "\n",
        "        # Resize the images to our requirement\n",
        "        padded = cv2.resize(padded, (28, 28))\n",
        "        chars.append((padded, (x,y,w,h)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7Lvs9gNc8SH"
      },
      "source": [
        "# Separate the boxes and chars into 2 different lists\n",
        "boxes = [b[1] for b in chars]\n",
        "test_chars = np.array([c[0] for c in chars], dtype=\"float32\")\n",
        "\n",
        "# Create a list of labels\n",
        "labels = list(range(0,10))\n",
        "for i in range(65, 91):\n",
        "    labels.append(chr(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7FKHk-KdQql"
      },
      "source": [
        "# Have the model predict the output of our characters\n",
        "preds = model.predict(test_chars.reshape((test_chars.shape[0], 28,28,1)), verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQBXjya9qfKc"
      },
      "source": [
        "# Draw the outputs onto the original image\n",
        "img2 = img.copy()\n",
        "for (pred, (x,y,w,h)) in zip(preds, boxes):\n",
        "    i = np.argmax(pred)\n",
        "    prob = pred[i]\n",
        "    label = labels[i]\n",
        "    cv2.rectangle(img2, (x,y), (x+w, y+h), (255,0,0), 2)\n",
        "    cv2.putText(img2, str(label), (x-10, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,0,0), 2)\n",
        "\n",
        "cv2_imshow(img2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_YF1oER3hRtp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
